{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week #3\n",
    "## Метод опорных векторов\n",
    "### Опорные объекты\n",
    "\n",
    "Метод опорных векторов (Support Vector Machine, SVM) — один из видов линейных классификаторов. Функционал, который он оптимизирует, направлен на максимизацию ширины разделяющей полосы между классами. Из теории статистического обучения известно, что эта ширина тесно связана с обобщающей способностью алгоритма, а ее максимизация позволяет бороться с переобучением.\n",
    "\n",
    "Метод опорных векторов имеет еще одну особенность. Если преобразовать его оптимизационную задачу, то окажется, что итоговый классификатор можно представить как взвешенную сумму скалярных произведений данного объекта на объекты обучающей выборки.\n",
    "\n",
    "По сути, алгоритм делает предсказания на основе сходства нового объекта с объектами обучающей выборки. При этом, как правило, далеко не все коэффициенты оказываются ненулевыми. Это означает, что классификация делается на основе сходства лишь с частью обучающих объектов. Такие объекты называются опорными.\n",
    "\n",
    "Метод опорных векторов реализован в классе sklearn.svm.SVC.\n",
    "\n",
    "- Основными параметрами этого класса являются коэффициент С и тип ядра kernel. В данной задаче мы будем использовать линейное ядро — для этого нужно задать значение параметра kernel='linear'\n",
    "- Индексы опорных объектов обученного классификатора хранятся в поле support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Загрузите выборку из файла svm-data.csv. В нем записана двумерная выборка \n",
    "# (целевая переменная указана в первом столбце, признаки — во втором и третьем).\n",
    "data = pd.read_csv('data/svm-data.csv', header=None)\n",
    "X = data.ix[:,1:]\n",
    "y = data.ix[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обучите классификатор с линейным ядром, параметром C = 100000 и random_state=241. \n",
    "# Такое значение параметра нужно использовать, чтобы убедиться, что SVM работает \n",
    "# с выборкой как с линейно разделимой. При более низких значениях параметра алгоритм \n",
    "# будет настраиваться с учетом слагаемого в функционале, штрафующего за маленькие отступы, \n",
    "# из-за чего результат может не совпасть с решением классической задачи SVM для линейно \n",
    "# разделимой выборки.\n",
    "method = SVC(C=100000, kernel='linear', random_state=241).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 10\n"
     ]
    }
   ],
   "source": [
    "# Найдите номера объектов, которые являются опорными (нумерация с единицы). \n",
    "# Они будут являться ответом на задание. Обратите внимание, что в качестве ответа \n",
    "# нужно привести номера объектов в возрастающем порядке через запятую или пробел. \n",
    "# Нумерация начинается с 1.\n",
    "print ' '.join(str(x+1) for x in method.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ текстов\n",
    "\n",
    "Метод опорных векторов (Support Vector Machine, SVM) — один из видов линейных классификаторов. Функционал, который он оптимизирует, направлен на максимизацию ширины разделяющей полосы между классами. Из теории статистического обучения известно, что эта ширина тесно связана с обобщающей способностью алгоритма, а ее максимизация позволяет бороться с переобучением.\n",
    "\n",
    "Одна из причин популярности линейных методов заключается в том, что они хорошо работают на разреженных данных. Так называются выборки с большим количеством признаков, где на каждом объекте большинство признаков равны нулю. Разреженные данные возникают, например, при работе с текстами. Дело в том, что текст удобно кодировать с помощью \"мешка слов\" — формируется столько признаков, сколько всего уникальных слов встречается в текстах, и значение каждого признака равно числу вхождений в документ соответствующего слова. Ясно, что общее число различных слов в наборе текстов может достигать десятков тысяч, и при этом лишь небольшая их часть будет встречаться в одном конкретном тексте.\n",
    "\n",
    "Можно кодировать тексты хитрее, и записывать не количество вхождений слова в текст, а TF-IDF. Это показатель, который равен произведению двух чисел: TF (term frequency) и IDF (inverse document frequency). Первая равна отношению числа вхождений слова в документ к общей длине документа. Вторая величина зависит от того, в скольки документах выборки встречается это слово. Чем больше таких документов, тем меньше IDF. Таким образом, TF-IDF будет иметь высокое значение для тех слов, которые много раз встречаются в данном документе, и редко встречаются в остальных.\n",
    "\n",
    "Как мы уже говорили выше, линейные методы часто применяются для решения различных задач анализа текстов. В этом задании мы применим метод опорных векторов для определения того, к какой из тематик относится новость: атеизм или космос.\n",
    "\n",
    "Для начала вам потребуется загрузить данные. В этом задании мы воспользуемся одним из датасетов, доступных в scikit-learn'е — 20 newsgroups. Для этого нужно воспользоваться модулем datasets:\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "newsgroups = datasets.fetch_20newsgroups(subset='all', categories=['alt.atheism', 'sci.space'])\n",
    "\n",
    "После выполнения этого кода массив с текстами будет находиться в поле newsgroups.data, номер класса — в поле newsgroups.target.\n",
    "\n",
    "Одна из сложностей работы с текстовыми данными состоит в том, что для них нужно построить числовое представление. Одним из способов нахождения такого представления является вычисление TF-IDF. В Scikit-Learn это реализовано в классе sklearn.feature_extraction.text.TfidfVectorizer. Преобразование обучающей выборки нужно делать с помощью функции fit_transform, тестовой — с помощью transform.\n",
    "\n",
    "Реализация SVM-классификатора находится в классе sklearn.svm.SVC. Веса каждого признака у обученного классификатора хранятся в поле coef_. Чтобы понять, какому слову соответствует i-й признак, можно воспользоваться методом get_feature_names() у TfidfVectorizer:\n",
    "\n",
    "feature_mapping = vectorizer.get_feature_names()\n",
    "\n",
    "print feature_mapping[i]\n",
    "\n",
    "Подбор параметров удобно делать с помощью класса sklearn.grid_search.GridSearchCV (При использовании библиотеки scikit-learn версии 18.0.1 sklearn.model_selection.GridSearchCV). Пример использования:\n",
    "\n",
    "grid = {'C': np.power(10.0, np.arange(-5, 6))}\n",
    "\n",
    "cv = KFold(y.size, n_folds=5, shuffle=True, random_state=241)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', random_state=241)\n",
    "\n",
    "gs = GridSearchCV(clf, grid, scoring='accuracy', cv=cv)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "При использовании библиотеки scikit-learn версии 18.0.1 и выше KFold задаётся немного по-другому:\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=241)\n",
    "\n",
    "Первым аргументом в GridSearchCV передается классификатор, для которого будут подбираться значения параметров, вторым — словарь (dict), задающий сетку параметров для перебора. После того, как перебор окончен, можно проанализировать значения качества для всех значений параметров и выбрать наилучший вариант:\n",
    "\n",
    "for a in gs.grid_scores_:\n",
    "    #a.mean_validation_score — оценка качества по кросс-валидации\n",
    "    #a.parameters — значения параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Загрузите объекты из новостного датасета 20 newsgroups, относящиеся к категориям \n",
    "# \"космос\" и \"атеизм\" (инструкция приведена выше). Обратите внимание, что загрузка данных \n",
    "# может занять несколько минут\n",
    "data = datasets.fetch_20newsgroups(subset='all', categories=['alt.atheism', 'sci.space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 28382\n",
      "[u'do', u'do1', u'doable', u'dobbs', u'dobyns', u'doc', u'dock', u'docked', u'docking', u'docs', u'doctor', u'doctoral', u'doctors', u'doctrinaire', u'doctrinal', u'doctrine', u'doctrines', u'document', u'documentary', u'documentation', u'documented', u'documents', u'dod', u'dodge', u'doe', u'doees', u'doen', u'does', u'doesn', u'dof', u'dog', u'dogma', u'dogmas', u'dogmatic', u'dogs', u'doi', u'doing', u'doings', u'doink', u'dokas', u'dolan', u'dole', u'dollar', u'dollars', u'dolphin', u'dolphins', u'domain', u'domains', u'domestic', u'domesticated']\n"
     ]
    }
   ],
   "source": [
    "# Вычислите TF-IDF-признаки для всех текстов. Обратите внимание, что в этом задании \n",
    "# мы предлагаем вам вычислить TF-IDF по всем данным. При таком подходе получается, \n",
    "# что признаки на обучающем множестве используют информацию из тестовой выборки — \n",
    "# но такая ситуация вполне законна, поскольку мы не используем значения целевой переменной \n",
    "# из теста. На практике нередко встречаются ситуации, когда признаки объектов тестовой выборки \n",
    "# известны на момент обучения, и поэтому можно ими пользоваться при обучении алгоритма.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(data.data)\n",
    "y = data.target\n",
    "all_words = tfidf_vectorizer.get_feature_names()\n",
    "print 'Words: {}'.format(len(all_words))\n",
    "print all_words[10000:10050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=241, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=241, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([  1.00000e-05,   1.00000e-04,   1.00000e-03,   1.00000e-02,\n",
       "         1.00000e-01,   1.00000e+00,   1.00000e+01,   1.00000e+02,\n",
       "         1.00000e+03,   1.00000e+04,   1.00000e+05])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подберите минимальный лучший параметр C из множества [10^-5, 10^-4, ... 10^4, 10^5] \n",
    "# для SVM с линейным ядром (kernel='linear') при помощи кросс-валидации по 5 блокам. \n",
    "# Укажите параметр random_state=241 и для SVM, и для KFold. В качестве меры качества \n",
    "# используйте долю верных ответов (accuracy).\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=241)\n",
    "grid = {'C': np.power(10.0, np.arange(-5, 6))}\n",
    "method = SVC(kernel='linear', random_state=241)\n",
    "gs = GridSearchCV(method, grid, scoring='accuracy', cv=cv)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=241, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучите SVM по всей выборке с оптимальным параметром C, найденным на предыдущем шаге.\n",
    "best_method = gs.best_estimator_\n",
    "best_method.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: (28382,)\n",
      "[ 0.29258057 -0.12314757  0.         ...,  0.01972862  0.05831336\n",
      " -0.00297347]\n",
      "Indices of words with max abs weights: (10,)\n",
      "[22936 15606  5776 21850 23673 17802  5093  5088 12871 24019]\n",
      "10 most crucial words:\n",
      "atheism atheists bible god keith moon religion sci sky space\n"
     ]
    }
   ],
   "source": [
    "# Найдите 10 слов с наибольшим абсолютным значением веса (веса хранятся в поле \n",
    "# coef_ у svm.SVC). Они являются ответом на это задание. Укажите эти слова через запятую \n",
    "# или пробел, в нижнем регистре, в лексикографическом порядке.\n",
    "num = 10\n",
    "weights = best_method.coef_.A.squeeze()\n",
    "print 'Weights: {}'.format(weights.shape)\n",
    "print weights\n",
    "indices = np.argsort(np.abs(weights))[-num:] #[::-1] to reverse\n",
    "print 'Indices of words with max abs weights: {}'.format(indices.shape)\n",
    "print indices\n",
    "words = np.array(all_words)[indices]\n",
    "print '{} most crucial words:'.format(num)\n",
    "print ' '.join(w for w in sorted(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Логистическая регрессия\n",
    "### Логистическая регрессия\n",
    "\n",
    "Логистическая регрессия — один из видов линейных классификаторов. Одной из ее особенностей является возможность оценивания вероятностей классов, тогда как большинство линейных классификаторов могут выдавать только номера классов.\n",
    "\n",
    "Логистическая регрессия использует достаточно сложный функционал качества, который не допускает записи решения в явном виде (в отличие от, например, линейной регрессии). Тем не менее, логистическую регрессию можно настраивать с помощью градиентного спуска.\n",
    "\n",
    "Мы будем работать с выборкой, содержащей два признака. Будем считать, что ответы лежат в множестве {-1, 1}. Для настройки логистической регрессии мы будем решать следующую задачу:\n",
    "\n",
    "Здесь xi1 и xi2 — значение первого и второго признаков соответственно на объекте xi. В этом задании мы будем рассматривать алгоритмы без свободного члена, чтобы упростить работу.\n",
    "\n",
    "Градиентный шаг для весов будет заключаться в одновременном обновлении весов w1 и w2 по следующим формулам (проверьте сами, что здесь действительно выписана производная нашего функционала):\n",
    "\n",
    "Линейные методы могут переобучаться и давать плохое качество из-за различных проблем в данных: мультиколлинеарности, зашумленности и т.д. Чтобы избежать этого, следует использовать регуляризацию — она позволяет понизить сложность модели и не допустить переобучения. Сила регуляризации определяется коэффициентом C в формулах, указанных выше.\n",
    "\n",
    "В этом задании мы предлагаем вам самостоятельно реализовать градиентный спуск.\n",
    "\n",
    "В качестве метрики качества будем использовать AUC-ROC (Area Under ROC-Curve). Она предназначена для алгоритмов бинарной классификации, выдающих оценку принадлежности объекта к одному из классов. По сути, значение этой метрики является агрегацией показателей качества всех алгоритмов, которые можно получить, выбирая какой-либо порог для оценки принадлежности.\n",
    "\n",
    "В Scikit-Learn метрика AUC реализована функцией sklearn.metrics.roc_auc_score. В качестве первого аргумента ей передается вектор истинных ответов, в качестве второго — вектор с оценками принадлежности объектов к первому классу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Загрузите данные из файла data-logistic.csv. Это двумерная выборка, \n",
    "# целевая переменная на которой принимает значения -1 или 1.\n",
    "data = pd.read_csv('data/data-logistic.csv', header=None)\n",
    "X = data.ix[:,1:].get_values()\n",
    "y = data.ix[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Убедитесь, что выше выписаны правильные формулы для градиентного спуска. \n",
    "# Обратите внимание, что мы используем полноценный градиентный спуск, а не его \n",
    "# стохастический вариант!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Реализуйте градиентный спуск для обычной и L2-регуляризованной (с коэффициентом \n",
    "# регуляризации 10) логистической регрессии. Используйте длину шага k=0.1. \n",
    "# В качестве начального приближения используйте вектор (0, 0).\n",
    "\n",
    "def gradient_descent_step(X, y, C, k, w):\n",
    "    L = X.shape[0]\n",
    "    M = X.shape[1]\n",
    "    assert(len(w) == M)\n",
    "    assert(y.shape[0] == L)\n",
    "\n",
    "    dot_product = [ np.dot(w, X[i,:]) for i in range(L) ]\n",
    "\n",
    "    w_new = np.array([ \n",
    "        w[j] \n",
    "        + k / L * np.sum(\n",
    "            [ y[i] * X[i,j] * (1 - 1 / (1 + math.exp(-y[i] * dot_product[i]))) for i in range(L) ]\n",
    "        )\n",
    "        - k * C * w[j] \n",
    "        for j in range(M) \n",
    "    ])\n",
    "    \n",
    "    return w_new\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, C, k, N, w):\n",
    "    w_old = w\n",
    "    for i in range(0, N):\n",
    "        w_new = gradient_descent_step(X=X, y=y, C=C, k=k, w=w_old)\n",
    "        dist = np.linalg.norm(w_old - w_new)\n",
    "        \n",
    "        print 'Iteration #{}: w = {} distance={}'.format(i, w_new, dist)\n",
    "        \n",
    "        if dist < 1e-5:\n",
    "            return w_new\n",
    "\n",
    "        w_old = w_new\n",
    "    \n",
    "    print 'Max number of iterations ({}) is reached'.format(N)\n",
    "    \n",
    "    return w_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With regulation:\n",
      "Iteration #0: w = [ 0.03573127  0.03245997] distance=0.0482739384539\n",
      "Iteration #1: w = [ 0.02669911  0.02275351] distance=0.0132587803327\n",
      "Iteration #2: w = [ 0.02905739  0.02532643] distance=0.00349018975742\n",
      "Iteration #3: w = [ 0.02842657  0.02463533] distance=0.000935717143569\n",
      "Iteration #4: w = [ 0.02859494  0.02481978] distance=0.00024974376394\n",
      "Iteration #5: w = [ 0.02854995  0.02477049] distance=6.67386427032e-05\n",
      "Iteration #6: w = [ 0.02856197  0.02478366] distance=1.78286589294e-05\n",
      "Iteration #7: w = [ 0.02855875  0.02478014] distance=4.76318913958e-06\n",
      "Without regulation:\n",
      "Iteration #0: w = [ 0.03573127  0.03245997] distance=0.0482739384539\n",
      "Iteration #1: w = [ 0.06243037  0.05521348] distance=0.0350794046557\n",
      "Iteration #2: w = [ 0.08343428  0.07202494] distance=0.026903330054\n",
      "Iteration #3: w = [ 0.10067273  0.08502955] distance=0.0215936087286\n",
      "Iteration #4: w = [ 0.11522227  0.0953854 ] distance=0.0178586914115\n",
      "Iteration #5: w = [ 0.1277369   0.10377852] distance=0.0150685196187\n",
      "Iteration #6: w = [ 0.13864803  0.11065262] distance=0.0128959691048\n",
      "Iteration #7: w = [ 0.14825909  0.11631519] distance=0.0111551471299\n",
      "Iteration #8: w = [ 0.15679401  0.1209907 ] distance=0.00973165622625\n",
      "Iteration #9: w = [ 0.16442411  0.12484951] distance=0.00855037490412\n",
      "Iteration #10: w = [ 0.17128422  0.1280248 ] distance=0.00755933435951\n",
      "Iteration #11: w = [ 0.17748271  0.13062309] distance=0.00672104200518\n",
      "Iteration #12: w = [ 0.18310823  0.1327311 ] distance=0.0060075097568\n",
      "Iteration #13: w = [ 0.18823429  0.13442043] distance=0.00539724837012\n",
      "Iteration #14: w = [ 0.19292253  0.13575085] distance=0.00487336574723\n",
      "Iteration #15: w = [ 0.19722515  0.13677278] distance=0.00442231657194\n",
      "Iteration #16: w = [ 0.20118668  0.137529  ] distance=0.00403305335197\n",
      "Iteration #17: w = [ 0.20484534  0.13805607] distance=0.00369643457089\n",
      "Iteration #18: w = [ 0.20823418  0.13838541] distance=0.00340480317187\n",
      "Iteration #19: w = [ 0.21138186  0.13854411] distance=0.00315168112772\n",
      "Iteration #20: w = [ 0.21431338  0.13855564] distance=0.00293154487835\n",
      "Iteration #21: w = [ 0.21705061  0.13844037] distance=0.0027396579129\n",
      "Iteration #22: w = [ 0.21961276  0.13821605] distance=0.00257194398433\n",
      "Iteration #23: w = [ 0.22201672  0.13789816] distance=0.00242488918482\n",
      "Iteration #24: w = [ 0.22427743  0.13750023] distance=0.00229546440353\n",
      "Iteration #25: w = [ 0.2264081  0.1370341] distance=0.00218106209911\n",
      "Iteration #26: w = [ 0.22842045  0.13651015] distance=0.00207944314472\n",
      "Iteration #27: w = [ 0.2303249   0.13593747] distance=0.00198869089109\n",
      "Iteration #28: w = [ 0.23213073  0.13532406] distance=0.00190717062258\n",
      "Iteration #29: w = [ 0.23384622  0.13467691] distance=0.0018334933066\n",
      "Iteration #30: w = [ 0.23547876  0.13400218] distance=0.00176648301057\n",
      "Iteration #31: w = [ 0.23703499  0.13330526] distance=0.00170514763936\n",
      "Iteration #32: w = [ 0.23852084  0.1325909 ] distance=0.00164865278544\n",
      "Iteration #33: w = [ 0.23994164  0.13186324] distance=0.00159629853332\n",
      "Iteration #34: w = [ 0.24130219  0.13112591] distance=0.00154749906004\n",
      "Iteration #35: w = [ 0.24260681  0.1303821 ] distance=0.00150176485206\n",
      "Iteration #36: w = [ 0.24385941  0.12963459] distance=0.00145868733428\n",
      "Iteration #37: w = [ 0.2450635   0.12888581] distance=0.00141792568818\n",
      "Iteration #38: w = [ 0.24622228  0.12813786] distance=0.00137919562731\n",
      "Iteration #39: w = [ 0.24733862  0.12739258] distance=0.00134225989904\n",
      "Iteration #40: w = [ 0.24841515  0.12665156] distance=0.00130692029084\n",
      "Iteration #41: w = [ 0.24945426  0.12591616] distance=0.00127301093415\n",
      "Iteration #42: w = [ 0.25045811  0.12518756] distance=0.00124039271736\n",
      "Iteration #43: w = [ 0.25142869  0.12446678] distance=0.00120894864003\n",
      "Iteration #44: w = [ 0.25236781  0.12375466] distance=0.00117857996041\n",
      "Iteration #45: w = [ 0.25327711  0.12305193] distance=0.00114920300826\n",
      "Iteration #46: w = [ 0.25415813  0.12235919] distance=0.00112074655323\n",
      "Iteration #47: w = [ 0.25501225  0.12167695] distance=0.00109314963521\n",
      "Iteration #48: w = [ 0.25584076  0.12100561] distance=0.00106635977776\n",
      "Iteration #49: w = [ 0.25664484  0.1203455 ] distance=0.00104033151856\n",
      "Iteration #50: w = [ 0.25742558  0.11969688] distance=0.00101502520139\n",
      "Iteration #51: w = [ 0.25818401  0.11905994] distance=0.000990405983647\n",
      "Iteration #52: w = [ 0.25892106  0.11843482] distance=0.000966443021325\n",
      "Iteration #53: w = [ 0.25963759  0.1178216 ] distance=0.0009431087997\n",
      "Iteration #54: w = [ 0.26033444  0.11722035] distance=0.00092037858381\n",
      "Iteration #55: w = [ 0.26101234  0.11663106] distance=0.000898229967166\n",
      "Iteration #56: w = [ 0.26167202  0.11605372] distance=0.000876642501041\n",
      "Iteration #57: w = [ 0.26231414  0.11548827] distance=0.000855597389823\n",
      "Iteration #58: w = [ 0.26293932  0.11493464] distance=0.0008350772405\n",
      "Iteration #59: w = [ 0.26354815  0.11439273] distance=0.000815065856491\n",
      "Iteration #60: w = [ 0.26414118  0.11386244] distance=0.000795548067821\n",
      "Iteration #61: w = [ 0.26471893  0.11334363] distance=0.000776509591046\n",
      "Iteration #62: w = [ 0.26528191  0.11283616] distance=0.000757936913553\n",
      "Iteration #63: w = [ 0.26583057  0.11233987] distance=0.000739817197838\n",
      "Iteration #64: w = [ 0.26636536  0.11185461] distance=0.000722138202134\n",
      "Iteration #65: w = [ 0.26688671  0.1113802 ] distance=0.000704888214447\n",
      "Iteration #66: w = [ 0.26739501  0.11091646] distance=0.000688055997577\n",
      "Iteration #67: w = [ 0.26789066  0.11046323] distance=0.000671630743153\n",
      "Iteration #68: w = [ 0.26837401  0.1100203 ] distance=0.000655602033055\n",
      "Iteration #69: w = [ 0.26884542  0.10958749] distance=0.000639959806901\n",
      "Iteration #70: w = [ 0.26930522  0.10916462] distance=0.000624694334519\n",
      "Iteration #71: w = [ 0.26975375  0.10875149] distance=0.000609796192514\n",
      "Iteration #72: w = [ 0.2701913   0.10834791] distance=0.000595256244204\n",
      "Iteration #73: w = [ 0.27061817  0.10795368] distance=0.000581065622337\n",
      "Iteration #74: w = [ 0.27103466  0.10756862] distance=0.000567215714093\n",
      "Iteration #75: w = [ 0.27144104  0.10719253] distance=0.000553698147986\n",
      "Iteration #76: w = [ 0.27183757  0.10682524] distance=0.00054050478233\n",
      "Iteration #77: w = [ 0.27222451  0.10646653] distance=0.00052762769501\n",
      "Iteration #78: w = [ 0.27260212  0.10611625] distance=0.00051505917433\n",
      "Iteration #79: w = [ 0.27297062  0.10577419] distance=0.000502791710778\n",
      "Iteration #80: w = [ 0.27333026  0.10544018] distance=0.000490817989535\n",
      "Iteration #81: w = [ 0.27368125  0.10511404] distance=0.000479130883629\n",
      "Iteration #82: w = [ 0.27402383  0.10479559] distance=0.000467723447635\n",
      "Iteration #83: w = [ 0.27435819  0.10448466] distance=0.000456588911818\n",
      "Iteration #84: w = [ 0.27468455  0.10418109] distance=0.000445720676681\n",
      "Iteration #85: w = [ 0.2750031  0.1038847] distance=0.000435112307845\n",
      "Iteration #86: w = [ 0.27531403  0.10359532] distance=0.000424757531218\n",
      "Iteration #87: w = [ 0.27561755  0.1033128 ] distance=0.000414650228427\n",
      "Iteration #88: w = [ 0.27591381  0.10303699] distance=0.000404784432464\n",
      "Iteration #89: w = [ 0.27620302  0.10276771] distance=0.000395154323536\n",
      "Iteration #90: w = [ 0.27648533  0.10250483] distance=0.000385754225089\n",
      "Iteration #91: w = [ 0.27676092  0.10224819] distance=0.000376578599996\n",
      "Iteration #92: w = [ 0.27702994  0.10199765] distance=0.000367622046882\n",
      "Iteration #93: w = [ 0.27729257  0.10175307] distance=0.000358879296586\n",
      "Iteration #94: w = [ 0.27754895  0.10151429] distance=0.000350345208744\n",
      "Iteration #95: w = [ 0.27779923  0.1012812 ] distance=0.000342014768481\n",
      "Iteration #96: w = [ 0.27804356  0.10105365] distance=0.000333883083214\n",
      "Iteration #97: w = [ 0.27828209  0.10083152] distance=0.000325945379542\n",
      "Iteration #98: w = [ 0.27851496  0.10061467] distance=0.000318197000245\n",
      "Iteration #99: w = [ 0.27874229  0.10040298] distance=0.000310633401353\n",
      "Iteration #100: w = [ 0.27896423  0.10019633] distance=0.000303250149311\n",
      "Iteration #101: w = [ 0.2791809   0.09999459] distance=0.000296042918217\n",
      "Iteration #102: w = [ 0.27939242  0.09979766] distance=0.000289007487141\n",
      "Iteration #103: w = [ 0.27959893  0.09960542] distance=0.000282139737506\n",
      "Iteration #104: w = [ 0.27980054  0.09941776] distance=0.000275435650557\n",
      "Iteration #105: w = [ 0.27999737  0.09923456] distance=0.000268891304878\n",
      "Iteration #106: w = [ 0.28018953  0.09905573] distance=0.000262502873989\n",
      "Iteration #107: w = [ 0.28037714  0.09888115] distance=0.000256266623998\n",
      "Iteration #108: w = [ 0.28056029  0.09871073] distance=0.000250178911317\n",
      "Iteration #109: w = [ 0.28073911  0.09854437] distance=0.000244236180435\n",
      "Iteration #110: w = [ 0.28091368  0.09838196] distance=0.000238434961753\n",
      "Iteration #111: w = [ 0.28108412  0.09822343] distance=0.000232771869465\n",
      "Iteration #112: w = [ 0.28125052  0.09806867] distance=0.000227243599506\n",
      "Iteration #113: w = [ 0.28141298  0.09791759] distance=0.00022184692754\n",
      "Iteration #114: w = [ 0.28157158  0.09777011] distance=0.000216578707007\n",
      "Iteration #115: w = [ 0.28172643  0.09762614] distance=0.000211435867216\n",
      "Iteration #116: w = [ 0.28187761  0.09748559] distance=0.000206415411489\n",
      "Iteration #117: w = [ 0.2820252   0.09734839] distance=0.000201514415345\n",
      "Iteration #118: w = [ 0.2821693   0.09721446] distance=0.00019673002474\n",
      "Iteration #119: w = [ 0.28230999  0.09708371] distance=0.000192059454343\n",
      "Iteration #120: w = [ 0.28244734  0.09695608] distance=0.000187499985859\n",
      "Iteration #121: w = [ 0.28258143  0.09683148] distance=0.000183048966389\n",
      "Iteration #122: w = [ 0.28271236  0.09670985] distance=0.000178703806844\n",
      "Iteration #123: w = [ 0.28284018  0.09659111] distance=0.000174461980379\n",
      "Iteration #124: w = [ 0.28296497  0.09647519] distance=0.000170321020886\n",
      "Iteration #125: w = [ 0.2830868   0.09636203] distance=0.000166278521508\n",
      "Iteration #126: w = [ 0.28320575  0.09625157] distance=0.000162332133202\n",
      "Iteration #127: w = [ 0.28332188  0.09614373] distance=0.000158479563334\n",
      "Iteration #128: w = [ 0.28343526  0.09603846] distance=0.000154718574302\n",
      "Iteration #129: w = [ 0.28354596  0.09593569] distance=0.000151046982206\n",
      "Iteration #130: w = [ 0.28365403  0.09583536] distance=0.000147462655541\n",
      "Iteration #131: w = [ 0.28375955  0.09573742] distance=0.000143963513923\n",
      "Iteration #132: w = [ 0.28386256  0.0956418 ] distance=0.000140547526857\n",
      "Iteration #133: w = [ 0.28396313  0.09554847] distance=0.00013721271252\n",
      "Iteration #134: w = [ 0.28406133  0.09545734] distance=0.000133957136584\n",
      "Iteration #135: w = [ 0.28415719  0.09536839] distance=0.000130778911068\n",
      "Iteration #136: w = [ 0.28425079  0.09528155] distance=0.000127676193214\n",
      "Iteration #137: w = [ 0.28434216  0.09519677] distance=0.000124647184393\n",
      "Iteration #138: w = [ 0.28443138  0.09511401] distance=0.00012169012904\n",
      "Iteration #139: w = [ 0.28451848  0.09503322] distance=0.000118803313609\n",
      "Iteration #140: w = [ 0.28460351  0.09495434] distance=0.000115985065561\n",
      "Iteration #141: w = [ 0.28468654  0.09487734] distance=0.000113233752372\n",
      "Iteration #142: w = [ 0.28476759  0.09480217] distance=0.00011054778057\n",
      "Iteration #143: w = [ 0.28484673  0.09472879] distance=0.000107925594787\n",
      "Iteration #144: w = [ 0.28492399  0.09465715] distance=0.000105365676845\n",
      "Iteration #145: w = [ 0.28499942  0.09458721] distance=0.000102866544857\n",
      "Iteration #146: w = [ 0.28507307  0.09451893] distance=0.000100426752352\n",
      "Iteration #147: w = [ 0.28514497  0.09445227] distance=9.80448874244e-05\n",
      "Iteration #148: w = [ 0.28521517  0.0943872 ] distance=9.5719571897e-05\n",
      "Iteration #149: w = [ 0.2852837   0.09432367] distance=9.34494605129e-05\n",
      "Iteration #150: w = [ 0.28535061  0.09426165] distance=9.1233240141e-05\n",
      "Iteration #151: w = [ 0.28541594  0.0942011 ] distance=8.90696290044e-05\n",
      "Iteration #152: w = [ 0.28547972  0.094142  ] distance=8.69573759253e-05\n",
      "Iteration #153: w = [ 0.28554199  0.09408429] distance=8.489525959e-05\n",
      "Iteration #154: w = [ 0.28560278  0.09402796] distance=8.28820878306e-05\n",
      "Iteration #155: w = [ 0.28566213  0.09397296] distance=8.09166969253e-05\n",
      "Iteration #156: w = [ 0.28572008  0.09391927] distance=7.89979509146e-05\n",
      "Iteration #157: w = [ 0.28577666  0.09386685] distance=7.7124740935e-05\n",
      "Iteration #158: w = [ 0.28583189  0.09381568] distance=7.52959845685e-05\n",
      "Iteration #159: w = [ 0.28588582  0.09376572] distance=7.35106252079e-05\n",
      "Iteration #160: w = [ 0.28593846  0.09371695] distance=7.17676314377e-05\n",
      "Iteration #161: w = [ 0.28598987  0.09366933] distance=7.00659964298e-05\n",
      "Iteration #162: w = [ 0.28604005  0.09362285] distance=6.84047373539e-05\n",
      "Iteration #163: w = [ 0.28608905  0.09357747] distance=6.67828948026e-05\n",
      "Iteration #164: w = [ 0.28613688  0.09353316] distance=6.51995322296e-05\n",
      "Iteration #165: w = [ 0.28618358  0.09348991] distance=6.36537354023e-05\n",
      "Iteration #166: w = [ 0.28622918  0.09344768] distance=6.21446118675e-05\n",
      "Iteration #167: w = [ 0.28627369  0.09340646] distance=6.06712904294e-05\n",
      "Iteration #168: w = [ 0.28631715  0.09336622] distance=5.92329206409e-05\n",
      "Iteration #169: w = [ 0.28635958  0.09332692] distance=5.78286723073e-05\n",
      "Iteration #170: w = [ 0.28640101  0.09328857] distance=5.64577350012e-05\n",
      "Iteration #171: w = [ 0.28644145  0.09325112] distance=5.51193175898e-05\n",
      "Iteration #172: w = [ 0.28648094  0.09321456] distance=5.38126477735e-05\n",
      "Iteration #173: w = [ 0.28651949  0.09317886] distance=5.25369716354e-05\n",
      "Iteration #174: w = [ 0.28655713  0.09314402] distance=5.1291553202e-05\n",
      "Iteration #175: w = [ 0.28659387  0.09311   ] distance=5.00756740139e-05\n",
      "Iteration #176: w = [ 0.28662975  0.09307679] distance=4.88886327078e-05\n",
      "Iteration #177: w = [ 0.28666478  0.09304436] distance=4.77297446075e-05\n",
      "Iteration #178: w = [ 0.28669897  0.09301271] distance=4.65983413254e-05\n",
      "Iteration #179: w = [ 0.28673236  0.0929818 ] distance=4.5493770374e-05\n",
      "Iteration #180: w = [ 0.28676495  0.09295163] distance=4.44153947853e-05\n",
      "Iteration #181: w = [ 0.28679678  0.09292218] distance=4.33625927409e-05\n",
      "Iteration #182: w = [ 0.28682784  0.09289342] distance=4.23347572101e-05\n",
      "Iteration #183: w = [ 0.28685818  0.09286535] distance=4.13312955973e-05\n",
      "Iteration #184: w = [ 0.28688779  0.09283794] distance=4.03516293969e-05\n",
      "Iteration #185: w = [ 0.2869167   0.09281118] distance=3.93951938579e-05\n",
      "Iteration #186: w = [ 0.28694493  0.09278506] distance=3.8461437655e-05\n",
      "Iteration #187: w = [ 0.28697249  0.09275955] distance=3.75498225691e-05\n",
      "Iteration #188: w = [ 0.2869994   0.09273465] distance=3.66598231742e-05\n",
      "Iteration #189: w = [ 0.28702567  0.09271034] distance=3.57909265327e-05\n",
      "Iteration #190: w = [ 0.28705131  0.09268661] distance=3.49426318974e-05\n",
      "Iteration #191: w = [ 0.28707635  0.09266344] distance=3.41144504213e-05\n",
      "Iteration #192: w = [ 0.2871008   0.09264082] distance=3.33059048735e-05\n",
      "Iteration #193: w = [ 0.28712466  0.09261873] distance=3.25165293627e-05\n",
      "Iteration #194: w = [ 0.28714797  0.09259717] distance=3.17458690669e-05\n",
      "Iteration #195: w = [ 0.28717072  0.09257613] distance=3.09934799698e-05\n",
      "Iteration #196: w = [ 0.28719293  0.09255558] distance=3.02589286033e-05\n",
      "Iteration #197: w = [ 0.28721461  0.09253551] distance=2.95417917961e-05\n",
      "Iteration #198: w = [ 0.28723578  0.09251593] distance=2.88416564289e-05\n",
      "Iteration #199: w = [ 0.28725645  0.0924968 ] distance=2.81581191945e-05\n",
      "Iteration #200: w = [ 0.28727663  0.09247813] distance=2.74907863648e-05\n",
      "Iteration #201: w = [ 0.28729633  0.09245991] distance=2.6839273562e-05\n",
      "Iteration #202: w = [ 0.28731556  0.09244211] distance=2.6203205537e-05\n",
      "Iteration #203: w = [ 0.28733434  0.09242474] distance=2.55822159511e-05\n",
      "Iteration #204: w = [ 0.28735268  0.09240778] distance=2.49759471647e-05\n",
      "Iteration #205: w = [ 0.28737058  0.09239122] distance=2.43840500297e-05\n",
      "Iteration #206: w = [ 0.28738805  0.09237505] distance=2.38061836882e-05\n",
      "Iteration #207: w = [ 0.28740511  0.09235927] distance=2.32420153744e-05\n",
      "Iteration #208: w = [ 0.28742177  0.09234386] distance=2.26912202228e-05\n",
      "Iteration #209: w = [ 0.28743803  0.09232882] distance=2.21534810799e-05\n",
      "Iteration #210: w = [ 0.28745391  0.09231413] distance=2.16284883208e-05\n",
      "Iteration #211: w = [ 0.28746941  0.09229979] distance=2.11159396704e-05\n",
      "Iteration #212: w = [ 0.28748454  0.09228579] distance=2.06155400281e-05\n",
      "Iteration #213: w = [ 0.28749932  0.09227213] distance=2.0127001298e-05\n",
      "Iteration #214: w = [ 0.28751375  0.09225878] distance=1.96500422216e-05\n",
      "Iteration #215: w = [ 0.28752783  0.09224576] distance=1.91843882155e-05\n",
      "Iteration #216: w = [ 0.28754158  0.09223304] distance=1.87297712129e-05\n",
      "Iteration #217: w = [ 0.287555    0.09222062] distance=1.82859295085e-05\n",
      "Iteration #218: w = [ 0.28756811  0.0922085 ] distance=1.78526076073e-05\n",
      "Iteration #219: w = [ 0.28758091  0.09219667] distance=1.74295560771e-05\n",
      "Iteration #220: w = [ 0.2875934   0.09218511] distance=1.70165314043e-05\n",
      "Iteration #221: w = [ 0.28760559  0.09217383] distance=1.66132958534e-05\n",
      "Iteration #222: w = [ 0.2876175   0.09216282] distance=1.62196173295e-05\n",
      "Iteration #223: w = [ 0.28762913  0.09215207] distance=1.58352692444e-05\n",
      "Iteration #224: w = [ 0.28764048  0.09214157] distance=1.5460030386e-05\n",
      "Iteration #225: w = [ 0.28765156  0.09213132] distance=1.509368479e-05\n",
      "Iteration #226: w = [ 0.28766238  0.09212132] distance=1.47360216157e-05\n",
      "Iteration #227: w = [ 0.28767294  0.09211155] distance=1.43868350243e-05\n",
      "Iteration #228: w = [ 0.28768325  0.09210201] distance=1.40459240599e-05\n",
      "Iteration #229: w = [ 0.28769332  0.0920927 ] distance=1.37130925334e-05\n",
      "Iteration #230: w = [ 0.28770315  0.09208361] distance=1.33881489096e-05\n",
      "Iteration #231: w = [ 0.28771274  0.09207474] distance=1.30709061968e-05\n",
      "Iteration #232: w = [ 0.28772211  0.09206607] distance=1.27611818382e-05\n",
      "Iteration #233: w = [ 0.28773126  0.09205761] distance=1.24587976073e-05\n",
      "Iteration #234: w = [ 0.28774019  0.09204935] distance=1.21635795049e-05\n",
      "Iteration #235: w = [ 0.28774891  0.09204129] distance=1.18753576585e-05\n",
      "Iteration #236: w = [ 0.28775742  0.09203342] distance=1.15939662245e-05\n",
      "Iteration #237: w = [ 0.28776573  0.09202574] distance=1.13192432925e-05\n",
      "Iteration #238: w = [ 0.28777385  0.09201823] distance=1.10510307922e-05\n",
      "Iteration #239: w = [ 0.28778177  0.09201091] distance=1.07891744016e-05\n",
      "Iteration #240: w = [ 0.2877895   0.09200376] distance=1.05335234588e-05\n",
      "Iteration #241: w = [ 0.28779705  0.09199677] distance=1.02839308745e-05\n",
      "Iteration #242: w = [ 0.28780442  0.09198996] distance=1.00402530474e-05\n",
      "Iteration #243: w = [ 0.28781162  0.0919833 ] distance=9.80234978163e-06\n"
     ]
    }
   ],
   "source": [
    "# Запустите градиентный спуск и доведите до сходимости (евклидово расстояние между \n",
    "# векторами весов на соседних итерациях должно быть не больше 1e-5). Рекомендуется \n",
    "# ограничить сверху число итераций десятью тысячами.\n",
    "C = 10\n",
    "k = 0.1\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "N = 10000\n",
    "w = np.array([0, 0])\n",
    "print 'With regulation:'\n",
    "w_opt_regul = gradient_descent(X=X, y=y, C=C, k=k, N=N, w=w)\n",
    "print 'Without regulation:'\n",
    "w_opt_noregul = gradient_descent(X=X, y=y, C=0, k=k, N=N, w=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC:\n",
      "0.926857142857 0.936285714286\n"
     ]
    }
   ],
   "source": [
    "# Какое значение принимает AUC-ROC на обучении без регуляризации и при ее использовании? \n",
    "# Эти величины будут ответом на задание. В качестве ответа приведите два числа через пробел. \n",
    "# Обратите внимание, что на вход функции roc_auc_score нужно подавать оценки вероятностей, \n",
    "# подсчитанные обученным алгоритмом. Для этого воспользуйтесь сигмоидной функцией: \n",
    "# a(x) = 1 / (1 + exp(-w1 x1 - w2 x2)).\n",
    "def get_auc_score(X, y_true, w):\n",
    "    L = X.shape[0]\n",
    "    y_predict = np.array([ 1 / (1 + math.exp(-np.dot(w, X[i,:]))) for i in range(L) ])\n",
    "    return roc_auc_score(y_true, y_predict)\n",
    "\n",
    "print 'AUC-ROC:'\n",
    "print '{} {}'.format(get_auc_score(X, y, w_opt_noregul), get_auc_score(X, y, w_opt_regul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Попробуйте поменять длину шага. Будет ли сходиться алгоритм, если делать более длинные шаги? \n",
    "# Как меняется число итераций при уменьшении длины шага?\n",
    "# w_opt1 = gradient_descent(X=X, y=y, C=C, k=0.2, N=N, w=w) # расходится при k>0.1\n",
    "# w_opt1 = gradient_descent(X=X, y=y, C=C, k=0.03, N=N, w=w) # дольше сходится при k<0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Попробуйте менять начальное приближение. Влияет ли оно на что-нибудь?\n",
    "# w_opt1 = gradient_descent(X=X, y=y, C=C, k=0.1, N=N, w=np.array([10, 10])) # не влияет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества\n",
    "### Метрики качества классификации\n",
    "\n",
    "В задачах классификации может быть много особенностей, влияющих на подсчет качества: различные цены ошибок, несбалансированность классов и т.д. Из-за этого существует большое количество метрик качества — каждая из них рассчитана на определенное сочетание свойств задачи и требований к ее решению.\n",
    "\n",
    "Меры качества классификации можно разбить на две большие группы: предназначенные для алгоритмов, выдающих номера классов, и для алгоритмов, выдающих оценки принадлежности к классам. К первой группе относятся доля правильных ответов, точность, полнота, F-мера. Ко второй — площади под ROC- или PR-кривой.\n",
    "\n",
    "Различные метрики качества реализованы в пакете sklearn.metrics. Конкретные функции указаны в инструкции по выполнению задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true  pred\n",
       "0     1     0\n",
       "1     1     1\n",
       "2     1     1\n",
       "3     0     0\n",
       "4     1     1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузите файл classification.csv. В нем записаны истинные классы объектов выборки \n",
    "# (колонка true) и ответы некоторого классификатора (колонка pred).\n",
    "data = pd.read_csv('data/classification.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 34 59 64\n"
     ]
    }
   ],
   "source": [
    "# Заполните таблицу ошибок классификации:\n",
    "# Для этого подсчитайте величины TP, FP, FN и TN согласно их определениям. \n",
    "# Например, FP — это количество объектов, имеющих класс 0, но отнесенных алгоритмом к классу 1. \n",
    "# Ответ в данном вопросе — четыре числа через пробел.\n",
    "TP = np.where(data.true + data.pred == 2)[0].size\n",
    "FP = np.nonzero(data.pred - data.true == 1)[0].size\n",
    "FN = np.nonzero(data.true - data.pred == 1)[0].size\n",
    "TN = np.nonzero(data.true + data.pred == 0)[0].size\n",
    "print '{} {} {} {}'.format(TP, FP, FN, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535 0.558441558442 0.421568627451 0.480446927374\n"
     ]
    }
   ],
   "source": [
    "# Посчитайте основные метрики качества классификатора:\n",
    "# - Accuracy (доля верно угаданных) — sklearn.metrics.accuracy_score\n",
    "# - Precision (точность) — sklearn.metrics.precision_score\n",
    "# - Recall (полнота) — sklearn.metrics.recall_score\n",
    "# - F-мера — sklearn.metrics.f1_score\n",
    "# В качестве ответа укажите эти четыре числа через пробел.\n",
    "accuracy = accuracy_score(data.true, data.pred)\n",
    "precision = precision_score(data.true, data.pred)\n",
    "recall = recall_score(data.true, data.pred)\n",
    "f1 = f1_score(data.true, data.pred)\n",
    "print '{} {} {} {}'.format(accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Имеется четыре обученных классификатора. В файле scores.csv записаны истинные классы и \n",
    "# значения степени принадлежности положительному классу для каждого классификатора на \n",
    "# некоторой выборке:\n",
    "# - для логистической регрессии — вероятность положительного класса (колонка score_logreg),\n",
    "# - для SVM — отступ от разделяющей поверхности (колонка score_svm),\n",
    "# - для метрического алгоритма — взвешенная сумма классов соседей (колонка score_knn),\n",
    "# - для решающего дерева — доля положительных объектов в листе (колонка score_tree).\n",
    "# Загрузите этот файл.\n",
    "data = pd.read_csv('data/scores.csv')\n",
    "data.head(5)\n",
    "columns = data.columns.get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['score_logreg' 0.719187675070028]\n"
     ]
    }
   ],
   "source": [
    "# Посчитайте площадь под ROC-кривой для каждого классификатора. \n",
    "# Какой классификатор имеет наибольшее значение метрики AUC-ROC (укажите название столбца)? \n",
    "# Воспользуйтесь функцией sklearn.metrics.roc_auc_score.\n",
    "roc_aucs = np.array([ roc_auc_score(data.true, data.ix[:,i]) for i in range(1, data.shape[1]) ])\n",
    "scores = np.vstack((columns[1:], roc_aucs)).transpose()\n",
    "print scores[np.argmax(scores[:,1]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max precision for >=70% recall:\n",
      "score_logreg 0.63025210084\n",
      "score_svm 0.622807017544\n",
      "score_knn 0.606557377049\n",
      "score_tree 0.651785714286\n"
     ]
    }
   ],
   "source": [
    "# Какой классификатор достигает наибольшей точности (Precision) при полноте (Recall) \n",
    "# не менее 70% ?\n",
    "# Чтобы получить ответ на этот вопрос, найдите все точки precision-recall-кривой \n",
    "# с помощью функции sklearn.metrics.precision_recall_curve. Она возвращает три массива: \n",
    "# precision, recall, thresholds. В них записаны точность и полнота при определенных порогах, \n",
    "# указанных в массиве thresholds. Найдите максимальной значение точности среди тех записей, \n",
    "# для которых полнота не меньше, чем 0.7.\n",
    "print 'Max precision for >=70% recall:'\n",
    "for i in range(1, data.shape[1]):\n",
    "    p, r, t = precision_recall_curve(data.true, data.ix[:,i])\n",
    "    p1 = np.max(p[np.where(r >= 0.7)])\n",
    "    print columns[i], p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ec60e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.close()\n",
    "\n",
    "for i in range(1, data.shape[1]):\n",
    "    p, r, _ = precision_recall_curve(data.true, data.ix[:,i])\n",
    "    plt.plot(r, p, label=columns[i])\n",
    "\n",
    "plt.title('PRC')\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
