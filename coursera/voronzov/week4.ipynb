{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week #4\n",
    "## Линейная регрессия\n",
    "### Прогноз оклада по описанию вакансии\n",
    "\n",
    "Линейные методы хорошо подходят для работы с разреженными данными — к таковым относятся, например, тексты. Это можно объяснить высокой скоростью обучения и небольшим количеством параметров, благодаря чему удается избежать переобучения.\n",
    "\n",
    "Линейная регрессия имеет несколько разновидностей в зависимости от того, какой регуляризатор используется. Мы будем работать с гребневой регрессией, где применяется квадратичный, или L2-регуляризатор.\n",
    "\n",
    "Для извлечения TF-IDF-признаков из текстов воспользуйтесь классом sklearn.feature_extraction.text.TfidfVectorizer.\n",
    "\n",
    "Для предсказания целевой переменной мы будем использовать гребневую регрессию, которая реализована в классе sklearn.linear_model.Ridge.\n",
    "\n",
    "Обратите внимание, что признаки LocationNormalized и ContractTime являются строковыми, и поэтому с ними нельзя работать напрямую. Такие нечисловые признаки с неупорядоченными значениями называют категориальными или номинальными. Типичный подход к их обработке — кодирование категориального признака с m возможными значениями с помощью m бинарных признаков. Каждый бинарный признак соответствует одному из возможных значений категориального признака и является индикатором того, что на данном объекте он принимает данное значение. Данный подход иногда называют one-hot-кодированием. Воспользуйтесь им, чтобы перекодировать признаки LocationNormalized и ContractTime. Он уже реализован в классе sklearn.feature_extraction.DictVectorizer. Пример использования:\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "enc = DictVectorizer()\n",
    "\n",
    "X_train_categ = enc.fit_transform(data_train[['LocationNormalized',         \n",
    "                                              'ContractTime']].to_dict('records'))\n",
    "\n",
    "X_test_categ = enc.transform(data_test[['LocationNormalized', \n",
    "                                        'ContractTime']].to_dict('records'))\n",
    "                                        \n",
    "                                        \n",
    "Вам понадобится производить замену пропущенных значений на специальные строковые величины (например, 'nan'). Для этого подходит следующий код:\n",
    "\n",
    "data_train['LocationNormalized'].fillna('nan', inplace=True)\n",
    "\n",
    "data_train['ContractTime'].fillna('nan', inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>International Sales Manager London ****k  ****...</td>\n",
       "      <td>London</td>\n",
       "      <td>permanent</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An ideal opportunity for an individual that ha...</td>\n",
       "      <td>London</td>\n",
       "      <td>permanent</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online Content and Brand Manager// Luxury Reta...</td>\n",
       "      <td>South East London</td>\n",
       "      <td>permanent</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A great local marketleader is seeking a perman...</td>\n",
       "      <td>Dereham</td>\n",
       "      <td>permanent</td>\n",
       "      <td>22500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Registered Nurse / RGN  Nursing Home for Young...</td>\n",
       "      <td>Sutton Coldfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     FullDescription LocationNormalized  \\\n",
       "0  International Sales Manager London ****k  ****...             London   \n",
       "1  An ideal opportunity for an individual that ha...             London   \n",
       "2  Online Content and Brand Manager// Luxury Reta...  South East London   \n",
       "3  A great local marketleader is seeking a perman...            Dereham   \n",
       "4  Registered Nurse / RGN  Nursing Home for Young...   Sutton Coldfield   \n",
       "\n",
       "  ContractTime  SalaryNormalized  \n",
       "0    permanent             33000  \n",
       "1    permanent             50000  \n",
       "2    permanent             40000  \n",
       "3    permanent             22500  \n",
       "4          NaN             20355  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузите данные об описаниях вакансий и соответствующих годовых зарплатах из файла \n",
    "# salary-train.csv (либо его заархивированную версию salary-train.zip).\n",
    "data_train = pd.read_csv('data/salary-train.csv')\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Проведите предобработку:\n",
    "# - Приведите тексты к нижнему регистру (text.lower()).\n",
    "# - Замените все, кроме букв и цифр, на пробелы — это облегчит дальнейшее разделение текста \n",
    "#   на слова. Для такой замены в строке text подходит следующий вызов: \n",
    "#       re.sub('[^a-zA-Z0-9]', ' ', text). \n",
    "#   Также можно воспользоваться методом replace у DataFrame, чтобы сразу преобразовать \n",
    "#   все тексты:\n",
    "#       train['FullDescription'] = \n",
    "#           train['FullDescription'].replace('[^a-zA-Z0-9]', ' ', regex = True)\n",
    "# - Примените TfidfVectorizer для преобразования текстов в векторы признаков. \n",
    "#   Оставьте только те слова, которые встречаются хотя бы в 5 объектах (параметр min_df \n",
    "#   у TfidfVectorizer).\n",
    "# - Замените пропуски в столбцах LocationNormalized и ContractTime на специальную строку 'nan'. \n",
    "#   Код для этого был приведен выше.\n",
    "# - Примените DictVectorizer для получения one-hot-кодирования признаков LocationNormalized \n",
    "#   и ContractTime.\n",
    "# - Объедините все полученные признаки в одну матрицу \"объекты-признаки\". Обратите внимание, \n",
    "#   что матрицы для текстов и категориальных признаков являются разреженными. Для объединения \n",
    "#   их столбцов нужно воспользоваться функцией scipy.sparse.hstack.\n",
    "data_train['FullDescription'] = data_train['FullDescription'].str.lower().replace('[^a-z0-9]', ' ', regex=True)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5)\n",
    "X_train1 = tfidf_vectorizer.fit_transform(data_train['FullDescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train['LocationNormalized'].fillna('nan', inplace=True)\n",
    "data_train['ContractTime'].fillna('nan', inplace=True)\n",
    "dict_vectorizer = DictVectorizer()\n",
    "X_train2 = dict_vectorizer.fit_transform(\n",
    "    data_train[['LocationNormalized', 'ContractTime']].to_dict('records')) \n",
    "\n",
    "X_train = hstack([X_train1, X_train2])\n",
    "y_train = data_train['SalaryNormalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Обучите гребневую регрессию с параметрами alpha=1 и random_state=241. \n",
    "# Целевая переменная записана в столбце SalaryNormalized.\n",
    "method = Ridge(alpha=1, random_state=241)\n",
    "a = method.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56555.6150015 37188.3244262\n"
     ]
    }
   ],
   "source": [
    "# Постройте прогнозы для двух примеров из файла salary-test-mini.csv. \n",
    "# Значения полученных прогнозов являются ответом на задание. Укажите их через пробел.\n",
    "data_test = pd.read_csv('data/salary-test-mini.csv')\n",
    "X_test1 = tfidf_vectorizer.transform(data_test['FullDescription'])\n",
    "data_train['LocationNormalized'].fillna('nan', inplace=True)\n",
    "data_train['ContractTime'].fillna('nan', inplace=True)\n",
    "X_test2 = dict_vectorizer.transform(\n",
    "    data_test[['LocationNormalized', 'ContractTime']].to_dict('records'))\n",
    "X_test = hstack([X_test1, X_test2])\n",
    "y_test_predict = method.predict(X_test)\n",
    "print ' '.join(str(y) for y in y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понижение размерности и метод главных компонент\n",
    "### Составление фондового индекса\n",
    "\n",
    "Метод главных компонент (principal component analysis, PCA) — это один из методов обучения без учителя, который позволяет сформировать новые признаки, являющиеся линейными комбинациями старых. При этом новые признаки строятся так, чтобы сохранить как можно больше дисперсии в данных. Иными словами, метод главных компонент понижает размерность данных оптимальным с точки зрения сохранения дисперсии способом.\n",
    "\n",
    "Основным параметром метода главных компонент является количество новых признаков. Как и в большинстве методов машинного обучения, нет четких рекомендаций по поводу выбора значения этого параметров. Один из подходов — выбирать минимальное число компонент, при котором объясняется не менее определенной доли дисперсии (это означает, что в выборке сохраняется данная доля от исходной дисперсии).\n",
    "\n",
    "В этом задании понадобится измерять схожесть двух наборов величин. Если имеется набор пар измерений (например, одна пара — предсказания двух классификаторов для одного и того же объекта), то охарактеризовать их зависимость друг от друга можно с помощью корреляции Пирсона. Она принимает значения от -1 до 1 и показывает, насколько данные величины линейно зависимы. Если корреляция равна -1 или 1, то величины линейно выражаются друг через друга. Если она равна нулю, то линейная зависимость между величинами отсутствует.\n",
    "\n",
    "В этом задании мы будем работать с данными о стоимостях акций 30 крупнейших компаний США. На основе этих данных можно оценить состояние экономики, например, с помощью индекса Доу-Джонса. Со временем состав компаний, по которым строится индекс, меняется. Для набора данных был взят период с 23.09.2013 по 18.03.2015, в котором набор компаний был фиксирован (подробнее почитать о составе можно по ссылке из материалов).\n",
    "\n",
    "Одним из существенных недостатков индекса Доу-Джонса является способ его вычисления — при подсчёте индекса цены входящих в него акций складываются, а потом делятся на поправочный коэффициент. В результате, даже если одна компания заметно меньше по капитализации, чем другая, но стоимость одной её акции выше, то она сильнее влияет на индекс. Даже большое процентное изменение цены относительно дешёвой акции может быть нивелировано незначительным в процентном отношении изменением цены более дорогой акции.\n",
    "\n",
    "Метод главных компонент реализован в пакете scikit-learn в модуле decomposition в классе PCA. Основным параметром является количество компонент (n_components). Для обученного преобразования этот класс позволяет вычислять различные характеристики. Например, поле explained_variance_ratio_ содержит процент дисперсии, который объясняет каждая компонента. Поле components_ содержит информацию о том, какой вклад вносят признаки в компоненты. Чтобы применить обученное преобразование к данным, можно воспользоваться методом transform.\n",
    "\n",
    "Для нахождения коэффициента корреляции Пирсона можно воспользоваться функцией corrcoef из пакета numpy.\n",
    "\n",
    "Ссылки: \n",
    "\n",
    "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average\n",
    "https://en.wikipedia.org/wiki/Historical_components_of_the_Dow_Jones_Industrial_Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AXP</th>\n",
       "      <th>BA</th>\n",
       "      <th>CAT</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>CVX</th>\n",
       "      <th>DD</th>\n",
       "      <th>DIS</th>\n",
       "      <th>GE</th>\n",
       "      <th>GS</th>\n",
       "      <th>HD</th>\n",
       "      <th>...</th>\n",
       "      <th>PFE</th>\n",
       "      <th>PG</th>\n",
       "      <th>T</th>\n",
       "      <th>TRV</th>\n",
       "      <th>UNH</th>\n",
       "      <th>UTX</th>\n",
       "      <th>V</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WMT</th>\n",
       "      <th>XOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-09-23</th>\n",
       "      <td>76.440002</td>\n",
       "      <td>117.510002</td>\n",
       "      <td>85.029999</td>\n",
       "      <td>24.270000</td>\n",
       "      <td>125.519997</td>\n",
       "      <td>59.409999</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>24.280001</td>\n",
       "      <td>165.250000</td>\n",
       "      <td>75.910004</td>\n",
       "      <td>...</td>\n",
       "      <td>28.799999</td>\n",
       "      <td>79.279999</td>\n",
       "      <td>34.220001</td>\n",
       "      <td>86.379997</td>\n",
       "      <td>71.820000</td>\n",
       "      <td>109.419998</td>\n",
       "      <td>196.240005</td>\n",
       "      <td>47.980000</td>\n",
       "      <td>76.419998</td>\n",
       "      <td>87.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-24</th>\n",
       "      <td>76.070000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>85.110001</td>\n",
       "      <td>24.139999</td>\n",
       "      <td>124.489998</td>\n",
       "      <td>59.319997</td>\n",
       "      <td>64.320000</td>\n",
       "      <td>24.320000</td>\n",
       "      <td>162.970001</td>\n",
       "      <td>76.040001</td>\n",
       "      <td>...</td>\n",
       "      <td>28.709999</td>\n",
       "      <td>78.620003</td>\n",
       "      <td>34.090000</td>\n",
       "      <td>85.870003</td>\n",
       "      <td>72.320000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>193.339996</td>\n",
       "      <td>47.270000</td>\n",
       "      <td>75.750000</td>\n",
       "      <td>87.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-25</th>\n",
       "      <td>75.989998</td>\n",
       "      <td>118.510002</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>24.430000</td>\n",
       "      <td>124.070000</td>\n",
       "      <td>59.319997</td>\n",
       "      <td>64.449997</td>\n",
       "      <td>24.230000</td>\n",
       "      <td>162.309998</td>\n",
       "      <td>75.519997</td>\n",
       "      <td>...</td>\n",
       "      <td>28.490000</td>\n",
       "      <td>77.720001</td>\n",
       "      <td>34.049999</td>\n",
       "      <td>85.980003</td>\n",
       "      <td>71.980003</td>\n",
       "      <td>109.260002</td>\n",
       "      <td>191.559998</td>\n",
       "      <td>46.950001</td>\n",
       "      <td>74.650002</td>\n",
       "      <td>87.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-26</th>\n",
       "      <td>76.320000</td>\n",
       "      <td>119.379997</td>\n",
       "      <td>84.199997</td>\n",
       "      <td>23.770000</td>\n",
       "      <td>123.489998</td>\n",
       "      <td>59.509996</td>\n",
       "      <td>65.239998</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>162.289993</td>\n",
       "      <td>76.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.520000</td>\n",
       "      <td>78.050003</td>\n",
       "      <td>34.230000</td>\n",
       "      <td>85.830002</td>\n",
       "      <td>72.160004</td>\n",
       "      <td>109.660004</td>\n",
       "      <td>193.559998</td>\n",
       "      <td>47.669998</td>\n",
       "      <td>74.620003</td>\n",
       "      <td>87.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-27</th>\n",
       "      <td>75.889999</td>\n",
       "      <td>118.739998</td>\n",
       "      <td>83.800003</td>\n",
       "      <td>23.330000</td>\n",
       "      <td>122.639999</td>\n",
       "      <td>59.009995</td>\n",
       "      <td>65.190002</td>\n",
       "      <td>24.049999</td>\n",
       "      <td>159.850006</td>\n",
       "      <td>75.959999</td>\n",
       "      <td>...</td>\n",
       "      <td>28.879999</td>\n",
       "      <td>77.209999</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>85.410004</td>\n",
       "      <td>71.989998</td>\n",
       "      <td>109.360001</td>\n",
       "      <td>193.050003</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>74.360001</td>\n",
       "      <td>86.900002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AXP          BA        CAT       CSCO         CVX  \\\n",
       "date                                                                  \n",
       "2013-09-23  76.440002  117.510002  85.029999  24.270000  125.519997   \n",
       "2013-09-24  76.070000  119.000000  85.110001  24.139999  124.489998   \n",
       "2013-09-25  75.989998  118.510002  84.500000  24.430000  124.070000   \n",
       "2013-09-26  76.320000  119.379997  84.199997  23.770000  123.489998   \n",
       "2013-09-27  75.889999  118.739998  83.800003  23.330000  122.639999   \n",
       "\n",
       "                   DD        DIS         GE          GS         HD    ...      \\\n",
       "date                                                                  ...       \n",
       "2013-09-23  59.409999  64.750000  24.280001  165.250000  75.910004    ...       \n",
       "2013-09-24  59.319997  64.320000  24.320000  162.970001  76.040001    ...       \n",
       "2013-09-25  59.319997  64.449997  24.230000  162.309998  75.519997    ...       \n",
       "2013-09-26  59.509996  65.239998  24.250000  162.289993  76.070000    ...       \n",
       "2013-09-27  59.009995  65.190002  24.049999  159.850006  75.959999    ...       \n",
       "\n",
       "                  PFE         PG          T        TRV        UNH         UTX  \\\n",
       "date                                                                            \n",
       "2013-09-23  28.799999  79.279999  34.220001  86.379997  71.820000  109.419998   \n",
       "2013-09-24  28.709999  78.620003  34.090000  85.870003  72.320000  110.000000   \n",
       "2013-09-25  28.490000  77.720001  34.049999  85.980003  71.980003  109.260002   \n",
       "2013-09-26  28.520000  78.050003  34.230000  85.830002  72.160004  109.660004   \n",
       "2013-09-27  28.879999  77.209999  33.980000  85.410004  71.989998  109.360001   \n",
       "\n",
       "                     V         VZ        WMT        XOM  \n",
       "date                                                     \n",
       "2013-09-23  196.240005  47.980000  76.419998  87.750000  \n",
       "2013-09-24  193.339996  47.270000  75.750000  87.360001  \n",
       "2013-09-25  191.559998  46.950001  74.650002  87.139999  \n",
       "2013-09-26  193.559998  47.669998  74.620003  87.070000  \n",
       "2013-09-27  193.050003  47.000000  74.360001  86.900002  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузите данные close_prices.csv. В этом файле приведены цены акций 30 компаний \n",
    "# на закрытии торгов за каждый день периода.\n",
    "data_train = pd.read_csv('data/close_prices.csv', index_col='date')\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.738971182715\n",
      "2 0.849042872814\n",
      "3 0.898993755584\n",
      "4 0.927742953784\n",
      "5 0.949897432663\n",
      "6 0.969213199174\n",
      "7 0.975961730737\n",
      "8 0.982102640924\n",
      "9 0.985308582609\n",
      "10 0.988364692911\n"
     ]
    }
   ],
   "source": [
    "# На загруженных данных обучите преобразование PCA с числом компоненты равным 10. \n",
    "# Скольких компонент хватит, чтобы объяснить 90% дисперсии?\n",
    "method = PCA(n_components=10)\n",
    "data_train_new = method.fit_transform(data_train)\n",
    "# print method.components_ # вклад признаков в главные компоненты\n",
    "# print method.explained_variance_ratio_ # процент дисперсии, который объясняет каждая компонента\n",
    "\n",
    "for i in range(method.explained_variance_ratio_.shape[0]):\n",
    "    print (i+1), np.sum(method.explained_variance_ratio_[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Примените построенное преобразование к исходным данным и возьмите значения первой компоненты.\n",
    "comp0 = method.transform(data_train)[:,0]\n",
    "# comp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.90965222]\n",
      " [ 0.90965222  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Загрузите информацию об индексе Доу-Джонса из файла djia_index.csv. \n",
    "# Чему равна корреляция Пирсона между первой компонентой и индексом Доу-Джонса?\n",
    "dj = pd.read_csv('data/djia_index.csv', index_col='date')\n",
    "pirson_coef = np.corrcoef(comp0, dj['^DJI'])\n",
    "print pirson_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP' 'BA' 'CAT' 'CSCO' 'CVX' 'DD' 'DIS' 'GE' 'GS' 'HD' 'IBM' 'INTC' 'JNJ'\n",
      " 'JPM' 'KO' 'MCD' 'MMM' 'MRK' 'MSFT' 'NKE' 'PFE' 'PG' 'T' 'TRV' 'UNH' 'UTX'\n",
      " 'V' 'VZ' 'WMT' 'XOM']\n",
      "Company with most weight: V\n",
      "Visa\n"
     ]
    }
   ],
   "source": [
    "# Какая компания имеет наибольший вес в первой компоненте? Укажите ее название с большой буквы.\n",
    "idx_max = np.argmax(method.components_[0])\n",
    "print data_train.columns.values\n",
    "print 'Company with most weight:', data_train.columns.values[idx_max]\n",
    "print 'Visa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
